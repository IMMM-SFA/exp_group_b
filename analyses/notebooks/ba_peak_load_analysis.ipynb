{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd40f4f4-5a80-40b7-94b3-aed136daa9d8",
   "metadata": {},
   "source": [
    "# Process and Plot the Peak Historical Loads by Balancing Authority\n",
    "\n",
    "This notebook analyzes the historical time series of extreme loads (summer and winter) by BA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e597e9-538f-41d4-afa9-a268b10ab1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by importing the packages we need:\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from glob import glob\n",
    "from matplotlib import pyplot\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795f6fe5-0769-4298-887e-7c4cffc64a0d",
   "metadata": {},
   "source": [
    "## Set the Directory Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd887600-32da-46dc-b411-fa63c75f564f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Identify the data input and output directories:\n",
    "load_data_input_dir =  '/Users/burl878/Documents/IMMM/Data/TELL/Production_Runs/tell_data/outputs/tell_output/historic/'\n",
    "temp_data_input_dir =  '/Volumes/LaCie/Big_Data/wrf_to_tell/wrf_tell_counties_output/historic/'\n",
    "metadata_input_dir =  '/Users/burl878/Documents/IMMM/Data/TELL_Input_Data/tell_raw_data/County_Shapefiles/'\n",
    "data_output_dir =  '/Users/burl878/Documents/IMMM/Data/TELL/Production_Runs/tell_data/outputs/postprocessed/ba_load_time_series/'\n",
    "image_output_dir =  '/Users/burl878/Documents/IMMM/Images/TELL/Analysis/BA_Peak_Loads/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc6dcda-c289-4a2d-b6b5-000a8415c651",
   "metadata": {},
   "source": [
    "## Set the BA to Process and Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46a8ff3-8da3-405e-8838-fae1ae4a53df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ba_to_process = 'BPAT'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afedcfd4-7a50-42aa-8479-8474ecbb85cf",
   "metadata": {},
   "source": [
    "## Process the Historical Peak Loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b04f656-5fab-4fcb-86db-63c64db9c2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to process the time series of historical peak loads:\n",
    "def process_ba_historical_peak_loads(ba_to_process: str, load_data_input_dir: str, data_output_dir: str):\n",
    "    \n",
    "    #Initiate a counter and empty dataframe to store the results:\n",
    "    counter = 0;\n",
    "    stats_df = pd.DataFrame()\n",
    "    \n",
    "    # Loop over the years and find the maximum summer and winter load in each year:\n",
    "    for year in range(1980,2020,1):\n",
    "        # Iterate the counter by one:\n",
    "        counter = counter + 1 \n",
    "\n",
    "        # Read in the historical interconnection loads file:\n",
    "        ba_df = pd.read_csv((load_data_input_dir + '/' + str(year) + '/' + 'TELL_Balancing_Authority_Hourly_Load_Data_' + str(year) + '_Scaled_' + str(year) + '.csv'))\n",
    "    \n",
    "        # Subset to just the data for the BA being processed:\n",
    "        ba_df = ba_df.loc[ba_df['BA_Code'] == ba_to_process]\n",
    "    \n",
    "        # Set the time value as a datetime variable:\n",
    "        ba_df['Datetime'] = pd.to_datetime(ba_df['Time_UTC'])\n",
    "        \n",
    "        # Add columns with the year and month values to be used in grouping:\n",
    "        ba_df['Year'] = ba_df['Datetime'].dt.strftime('%Y').astype(str).astype(int)\n",
    "        ba_df['Month'] = ba_df['Datetime'].dt.strftime('%m').astype(str).astype(int)\n",
    "    \n",
    "        # Compute the annual mean and standard deviation of the load:\n",
    "        ba_df['Year_Mean_Load'] = ba_df.groupby('Year')['Scaled_TELL_BA_Load_MWh'].transform('mean').round(2)\n",
    "        ba_df['Year_STD_Load'] = ba_df.groupby('Year')['Scaled_TELL_BA_Load_MWh'].transform('std').round(2)\n",
    "    \n",
    "        # Compute the hourly normalized load by subtracting the annual mean and dividing by the annual standard deviation:\n",
    "        ba_df['Normalized_Load'] = ((ba_df['Scaled_TELL_BA_Load_MWh'] - ba_df['Year_Mean_Load']) / ba_df['Year_STD_Load']).round(2)\n",
    "    \n",
    "        # Subset to just the columns we need:\n",
    "        ba_df = ba_df[['Time_UTC', 'Year', 'Month', 'Scaled_TELL_BA_Load_MWh', 'Normalized_Load', 'Year_Mean_Load']].copy()\n",
    "    \n",
    "        # Subset the data to just the summer and winter months:\n",
    "        winter_df = ba_df.loc[(ba_df['Month'] == 1) | (ba_df['Month'] == 2) | (ba_df['Month'] == 3) | (ba_df['Month'] == 10) | (ba_df['Month'] == 11) | (ba_df['Month'] == 12)]\n",
    "        summer_df = ba_df.loc[(ba_df['Month'] == 4) | (ba_df['Month'] == 5) | (ba_df['Month'] == 6) | (ba_df['Month'] == 7) | (ba_df['Month'] == 8) | (ba_df['Month'] == 9)]\n",
    "        \n",
    "        # Find the row of the maximum load for each subset:\n",
    "        all_max_index = ba_df['Scaled_TELL_BA_Load_MWh'].idxmax()\n",
    "        winter_max_index = winter_df['Scaled_TELL_BA_Load_MWh'].idxmax()\n",
    "        summer_max_index = summer_df['Scaled_TELL_BA_Load_MWh'].idxmax()\n",
    "                \n",
    "        # Put the statistics in a new dataframe:\n",
    "        stats_df.loc[counter, 'Year'] = str(year)\n",
    "        stats_df.loc[counter, 'Mean_Load_MWh'] = ba_df['Year_Mean_Load'].mean().round(2)\n",
    "        stats_df.loc[counter, 'All_Max_Time'] = ba_df.loc[all_max_index, 'Time_UTC']\n",
    "        stats_df.loc[counter, 'All_Max_Load_MWh'] = ba_df.loc[all_max_index, 'Scaled_TELL_BA_Load_MWh']\n",
    "        stats_df.loc[counter, 'All_Max_Load_Norm'] = ba_df.loc[all_max_index, 'Normalized_Load']\n",
    "        stats_df.loc[counter, 'Win_Max_Time'] = winter_df.loc[winter_max_index, 'Time_UTC']\n",
    "        stats_df.loc[counter, 'Win_Max_Load_MWh'] = winter_df.loc[winter_max_index, 'Scaled_TELL_BA_Load_MWh']\n",
    "        stats_df.loc[counter, 'Win_Max_Load_Norm'] = winter_df.loc[winter_max_index, 'Normalized_Load']\n",
    "        stats_df.loc[counter, 'Sum_Max_Time'] = summer_df.loc[summer_max_index, 'Time_UTC']\n",
    "        stats_df.loc[counter, 'Sum_Max_Load_MWh'] = summer_df.loc[summer_max_index, 'Scaled_TELL_BA_Load_MWh']\n",
    "        stats_df.loc[counter, 'Sum_Max_Load_Norm'] = summer_df.loc[summer_max_index, 'Normalized_Load']\n",
    "\n",
    "        # Clean up and move to the next year:\n",
    "        del ba_df, winter_df, summer_df, all_max_index, winter_max_index, summer_max_index\n",
    "        \n",
    "    # Write out the time series dataframe to a .csv file:\n",
    "    stats_df.to_csv((os.path.join(data_output_dir + ba_to_process + '_Peak_Load_Statistics.csv')), sep=',', index=False)\n",
    "    \n",
    "    return stats_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38e6c6d-8983-4b44-8901-68b335b9c30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = process_ba_historical_peak_loads(ba_to_process = ba_to_process, \n",
    "                                            load_data_input_dir = load_data_input_dir, \n",
    "                                            data_output_dir = data_output_dir)\n",
    "\n",
    "stats_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4167cba4-7ad9-4752-964d-933f1a1c4c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot the time series of extreme loads by BA:\n",
    "def plot_ba_historical_peak_loads(ba_to_process: str, data_output_dir: str, image_output_dir: str, image_resolution: int, save_images=False):\n",
    "    \n",
    "    # Check to see if the output already exist and if not then process it:\n",
    "    if os.path.isfile((os.path.join(data_output_dir + ba_to_process + '_Peak_Load_Statistics.csv'))) == True:\n",
    "       # Load in the pre-processed data:\n",
    "       stats_df = pd.read_csv((os.path.join(data_output_dir + ba_to_process + '_Peak_Load_Statistics.csv')))\n",
    "    else:\n",
    "       stats_df = process_ba_historical_peak_loads(ba_to_process = ba_to_process, load_data_input_dir = load_data_input_dir, data_output_dir = data_output_dir)\n",
    "    \n",
    "    # Find the four highest values:\n",
    "    winter_peak_raw = stats_df.nlargest(4, 'Win_Max_Load_MWh')\n",
    "    winter_peak_norm = stats_df.nlargest(4, 'Win_Max_Load_Norm')\n",
    "    summer_peak_raw = stats_df.nlargest(4, 'Sum_Max_Load_MWh')\n",
    "    summer_peak_norm = stats_df.nlargest(4, 'Sum_Max_Load_Norm')\n",
    "    \n",
    "    # Concatenate and print the dates of the four highest winter and summer load dates:\n",
    "    max_times = pd.concat([winter_peak_norm['Win_Max_Time'],summer_peak_norm['Sum_Max_Time']])\n",
    "    \n",
    "    # Make the plot:\n",
    "    plt.figure(figsize=(24, 15))\n",
    "    plt.rcParams['font.size'] = 16\n",
    "    \n",
    "    plt.subplot(211)\n",
    "    plt.plot(stats_df['Year'], stats_df['Win_Max_Load_MWh'], color='b', linestyle='-', label='Winter Peak Load', linewidth=3)\n",
    "    plt.plot(stats_df['Year'], stats_df['Sum_Max_Load_MWh'], color='r', linestyle='-', label='Summer Peak Load', linewidth=3)\n",
    "    plt.scatter(winter_peak_raw['Year'], winter_peak_raw['Win_Max_Load_MWh'], s=100, c='blue')\n",
    "    plt.scatter(summer_peak_raw['Year'], summer_peak_raw['Sum_Max_Load_MWh'], s=100, c='red')\n",
    "    plt.xlim([1979, 2020]); \n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='upper left', prop={'size': 14})\n",
    "    plt.ylabel('BA Load [MWh]')\n",
    "    plt.title((ba_to_process + ' Peak Loads: 1980-2019'))\n",
    "    \n",
    "    plt.subplot(212)\n",
    "    plt.plot(stats_df['Year'], stats_df['Win_Max_Load_Norm'], color='b', linestyle='-', label='Winter Peak Load', linewidth=3)\n",
    "    plt.plot(stats_df['Year'], stats_df['Sum_Max_Load_Norm'], color='r', linestyle='-', label='Summer Peak Load', linewidth=3)\n",
    "    plt.scatter(winter_peak_norm['Year'], winter_peak_norm['Win_Max_Load_Norm'], s=100, c='blue')\n",
    "    plt.scatter(summer_peak_norm['Year'], summer_peak_norm['Sum_Max_Load_Norm'], s=100, c='red')\n",
    "    plt.xlim([1979, 2020]); \n",
    "    plt.grid(True)\n",
    "    plt.xlabel('Year');\n",
    "    plt.ylabel('Annually Normalized BA Load')\n",
    "    plt.title(('Normalized Peak Loads'))\n",
    "    \n",
    "    # If the \"save_images\" flag is set to true then save the plot to a .png file:\n",
    "    if save_images == True:\n",
    "       filename = (ba_to_process + '_Peak_Load_Time_Series.png')\n",
    "       plt.savefig(os.path.join(image_output_dir, filename), dpi=image_resolution, bbox_inches='tight', facecolor='white')\n",
    "    \n",
    "    return max_times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce93bba6-4708-4e08-b43b-369f379c584b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_times = plot_ba_historical_peak_loads(ba_to_process = ba_to_process, \n",
    "                                          data_output_dir = data_output_dir,\n",
    "                                          image_output_dir = image_output_dir, \n",
    "                                          image_resolution = 300, \n",
    "                                          save_images = True)\n",
    "\n",
    "max_times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab44ef7-ecef-486c-a090-7b108920e17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot the temperature during extreme load events:\n",
    "def plot_peak_load_temperatures(ba_to_process: str, metadata_input_dir: str, temp_data_input_dir: str, data_output_dir: str, image_output_dir: str, image_resolution: int, save_images=False):\n",
    "\n",
    "    # Read in the county shapefile and reassign the 'FIPS' variable as integers:\n",
    "    counties_df = gpd.read_file(os.path.join(metadata_input_dir, r'tl_2020_us_county.shp')).rename(columns={'GEOID': 'FIPS'})\n",
    "    counties_df['FIPS'] = counties_df['FIPS'].astype(int)\n",
    "    \n",
    "    if ba_to_process == 'BPAT':\n",
    "       cold1_temp = pd.read_csv((os.path.join(temp_data_input_dir + '2004/' + '2004_01_05_17_UTC_County_Mean_Meteorology.csv')))\n",
    "       cold1_temp['T2'] = (1.8 * (cold1_temp['T2'] - 273)) + 32\n",
    "       cold1_label = 'BPAT Winter Peak One: 5-Jan 2004 1700 UTC'\n",
    "       cold2_temp = pd.read_csv((os.path.join(temp_data_input_dir + '1989/' + '1989_02_02_17_UTC_County_Mean_Meteorology.csv')))\n",
    "       cold2_temp['T2'] = (1.8 * (cold2_temp['T2'] - 273)) + 32\n",
    "       cold2_label = 'BPAT Winter Peak Two: 2-Feb 1989 1700 UTC'\n",
    "       cold3_temp = pd.read_csv((os.path.join(temp_data_input_dir + '1983/' + '1983_12_23_17_UTC_County_Mean_Meteorology.csv')))\n",
    "       cold3_temp['T2'] = (1.8 * (cold3_temp['T2'] - 273)) + 32\n",
    "       cold3_label = 'BPAT Winter Peak Three: 23-Dec 1983 1700 UTC'\n",
    "       cold4_temp = pd.read_csv((os.path.join(temp_data_input_dir + '1982/' + '1982_01_06_17_UTC_County_Mean_Meteorology.csv')))\n",
    "       cold4_temp['T2'] = (1.8 * (cold4_temp['T2'] - 273)) + 32\n",
    "       cold4_label = 'BPAT Winter Peak Four: 6-Jan 1982 1700 UTC'\n",
    "    \n",
    "       warm1_temp = pd.read_csv((os.path.join(temp_data_input_dir + '1998/' + '1998_07_28_02_UTC_County_Mean_Meteorology.csv')))\n",
    "       warm1_temp['T2'] = (1.8 * (warm1_temp['T2'] - 273)) + 32\n",
    "       warm1_label = 'BPAT Summer Peak One: 28-Jul 1998 0200 UTC'\n",
    "       warm2_temp = pd.read_csv((os.path.join(temp_data_input_dir + '2006/' + '2006_07_24_01_UTC_County_Mean_Meteorology.csv')))\n",
    "       warm2_temp['T2'] = (1.8 * (warm2_temp['T2'] - 273)) + 32\n",
    "       warm2_label = 'BPAT Summer Peak Two: 24-Jul 2006 0100 UTC'\n",
    "       warm3_temp = pd.read_csv((os.path.join(temp_data_input_dir + '1981/' + '1981_08_10_02_UTC_County_Mean_Meteorology.csv')))\n",
    "       warm3_temp['T2'] = (1.8 * (warm3_temp['T2'] - 273)) + 32\n",
    "       warm3_label = 'BPAT Summer Peak Three: 10-Aug 1981 0200 UTC'\n",
    "       warm4_temp = pd.read_csv((os.path.join(temp_data_input_dir + '2009/' + '2009_07_29_02_UTC_County_Mean_Meteorology.csv')))\n",
    "       warm4_temp['T2'] = (1.8 * (warm4_temp['T2'] - 273)) + 32\n",
    "       warm4_label = 'BPAT Summer Peak Four: 29-Jul 2009 0200 UTC'\n",
    "    \n",
    "    # Merge the ba_mapping_df and counties_df together using county FIPS codes to join them:\n",
    "    cold1_df = counties_df.merge(cold1_temp, on='FIPS', how='left')\n",
    "    cold2_df = counties_df.merge(cold2_temp, on='FIPS', how='left')\n",
    "    cold3_df = counties_df.merge(cold3_temp, on='FIPS', how='left')\n",
    "    cold4_df = counties_df.merge(cold4_temp, on='FIPS', how='left')\n",
    "    warm1_df = counties_df.merge(warm1_temp, on='FIPS', how='left')\n",
    "    warm2_df = counties_df.merge(warm2_temp, on='FIPS', how='left')\n",
    "    warm3_df = counties_df.merge(warm3_temp, on='FIPS', how='left')\n",
    "    warm4_df = counties_df.merge(warm4_temp, on='FIPS', how='left')\n",
    "    \n",
    "    # Create the cold event figure:\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(25,10))\n",
    "    ax1 = cold1_df.plot(ax=axes[0, 0], column='T2', cmap='RdYlBu_r', vmin=-45, vmax=75, edgecolor='grey', linewidth=0.2, legend=True, legend_kwds={'label': ('Temperature ($^\\circ$F)'), 'orientation': 'vertical'})\n",
    "    ax1.set_title(cold1_label)\n",
    "    ax2 = cold2_df.plot(ax=axes[0, 1], column='T2', cmap='RdYlBu_r', vmin=-45, vmax=75, edgecolor='grey', linewidth=0.2, legend=True, legend_kwds={'label': ('Temperature ($^\\circ$F)'), 'orientation': 'vertical'})\n",
    "    ax2.set_title(cold2_label)\n",
    "    ax3 = cold3_df.plot(ax=axes[1, 0], column='T2', cmap='RdYlBu_r', vmin=-45, vmax=75, edgecolor='grey', linewidth=0.2, legend=True, legend_kwds={'label': ('Temperature ($^\\circ$F)'), 'orientation': 'vertical'})\n",
    "    ax3.set_title(cold3_label)\n",
    "    ax4 = cold4_df.plot(ax=axes[1, 1], column='T2', cmap='RdYlBu_r', vmin=-45, vmax=75, edgecolor='grey', linewidth=0.2, legend=True, legend_kwds={'label': ('Temperature ($^\\circ$F)'), 'orientation': 'vertical'})\n",
    "    ax4.set_title(cold4_label)\n",
    "    # If the \"save_images\" flag is set to true then save the plot to a .png file:\n",
    "    if save_images == True:\n",
    "       filename = (ba_to_process + '_Winter_Peak_Maps.png')\n",
    "       plt.savefig(os.path.join(image_output_dir, filename), dpi=image_resolution, bbox_inches='tight', facecolor='white')\n",
    "    \n",
    "    # Create the hot event figure:\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(25,10))\n",
    "    ax1 = warm1_df.plot(ax=axes[0, 0], column='T2', cmap='RdYlBu_r', vmin=45, vmax=115, edgecolor='grey', linewidth=0.2, legend=True, legend_kwds={'label': ('Temperature ($^\\circ$F)'), 'orientation': 'vertical'})\n",
    "    ax1.set_title(warm1_label)\n",
    "    ax2 = warm2_df.plot(ax=axes[0, 1], column='T2', cmap='RdYlBu_r', vmin=45, vmax=115, edgecolor='grey', linewidth=0.2, legend=True, legend_kwds={'label': ('Temperature ($^\\circ$F)'), 'orientation': 'vertical'})\n",
    "    ax2.set_title(warm2_label)\n",
    "    ax3 = warm3_df.plot(ax=axes[1, 0], column='T2', cmap='RdYlBu_r', vmin=45, vmax=115, edgecolor='grey', linewidth=0.2, legend=True, legend_kwds={'label': ('Temperature ($^\\circ$F)'), 'orientation': 'vertical'})\n",
    "    ax3.set_title(warm3_label)\n",
    "    ax4 = warm4_df.plot(ax=axes[1, 1], column='T2', cmap='RdYlBu_r', vmin=45, vmax=115, edgecolor='grey', linewidth=0.2, legend=True, legend_kwds={'label': ('Temperature ($^\\circ$F)'), 'orientation': 'vertical'})\n",
    "    ax4.set_title(warm4_label)\n",
    "    # If the \"save_images\" flag is set to true then save the plot to a .png file:\n",
    "    if save_images == True:\n",
    "       filename = (ba_to_process + '_Summer_Peak_Maps.png')\n",
    "       plt.savefig(os.path.join(image_output_dir, filename), dpi=image_resolution, bbox_inches='tight', facecolor='white')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7819cd9-ee7f-44df-bb33-f579dc3547cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_peak_load_temperatures(ba_to_process = ba_to_process,\n",
    "                            metadata_input_dir = metadata_input_dir,\n",
    "                            temp_data_input_dir = temp_data_input_dir,\n",
    "                            data_output_dir = data_output_dir,\n",
    "                            image_output_dir = image_output_dir, \n",
    "                            image_resolution = 300, \n",
    "                            save_images = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7935b68c-a505-4bc2-a5ca-81e0734676a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.9.15_std",
   "language": "python",
   "name": "py3.9.15_std"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
