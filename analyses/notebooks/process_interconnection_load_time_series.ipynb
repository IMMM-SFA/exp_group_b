{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd40f4f4-5a80-40b7-94b3-aed136daa9d8",
   "metadata": {},
   "source": [
    "# Process Interconnection Load Time Series\n",
    "\n",
    "This notebook process the time-series of historical hourly loads for each of the three electricity interconnections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27e597e9-538f-41d4-afa9-a268b10ab1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by importing the packages we need:\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795f6fe5-0769-4298-887e-7c4cffc64a0d",
   "metadata": {},
   "source": [
    "## Set the Directory Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bd887600-32da-46dc-b411-fa63c75f564f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Identify the data input and output directories:\n",
    "data_input_dir =  '/Users/burl878/Documents/IMMM/Data/TELL/Production_Runs/tell_data/'\n",
    "metadata_input_dir =  '/Users/burl878/Documents/IMMM/Data/TELL/Analysis/'\n",
    "data_output_dir =  '/Users/burl878/Documents/IMMM/Data/TELL/Production_Runs/tell_data/outputs/postprocessed/Interconnection_Load_Time_Series/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f055d71-411a-4b85-b0b8-5fd3f9059796",
   "metadata": {},
   "source": [
    "## Process the TELL Output Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "367300aa-77bc-4985-8b82-85290eae7502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the BA-to-Interconnection mapping file:\n",
    "ba_mapping = pd.read_csv(metadata_input_dir + 'BA_to_Interconnection_Mapping.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "947b41c7-c488-4716-99d1-c98f4a91b5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to process the time series of hourly load for each interconnection:\n",
    "def process_interconnection_load_time_series(data_input_dir: str, scenario: str):\n",
    "\n",
    "    # Set the start and end years to loop over:\n",
    "    if scenario == 'historic':\n",
    "       start_year = 1980; end_year = 2020; interval = 1\n",
    "    else:\n",
    "       start_year = 2020; end_year = 2100; interval = 5\n",
    "        \n",
    "    # Loop over the years from the start_year to the end_year:\n",
    "    for year in range(start_year, end_year, interval):\n",
    "        # Read in the TELL BA output file for that year and scenario:\n",
    "        tell_df = pd.read_csv(data_input_dir + 'outputs/tell_output/' + scenario + '/' + str(year) + '/TELL_Balancing_Authority_Hourly_Load_Data_' + str(year) + '_Scaled_' + str(year) + '.csv')\n",
    "             \n",
    "        # Merge in the interconnection mapping using common BA codes:\n",
    "        merged_df = pd.merge(tell_df, ba_mapping, on='BA_Code')\n",
    "            \n",
    "        # Sum the BA-level hourly loads into interconnection-level hourly loads:\n",
    "        merged_df['Interconnection_Load_MWh'] = merged_df.groupby(['Interconnection', 'Time_UTC'])['Scaled_TELL_BA_Load_MWh'].transform('sum').round(3)  \n",
    "            \n",
    "        # Only keep the columns we need and subset to the unique values:\n",
    "        merged_df = merged_df[['Time_UTC', 'Interconnection', 'Interconnection_Load_MWh']].drop_duplicates()\n",
    "            \n",
    "        # Compute the annual percentile for each hourly load value:\n",
    "        merged_df['Percentile'] = merged_df['Interconnection_Load_MWh'].rank(pct=True).round(3)\n",
    "            \n",
    "        # Store the output in a new dataframe:\n",
    "        if year == start_year:\n",
    "           output_df = merged_df\n",
    "        else:\n",
    "           output_df = pd.concat([output_df, merged_df])\n",
    "            \n",
    "        # Clean up:\n",
    "        del tell_df, merged_df\n",
    "            \n",
    "    # Write out the dataframe to a .csv file:\n",
    "    output_df.to_csv((os.path.join(data_output_dir, (scenario + '.csv'))), sep=',', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4b04f656-5fab-4fcb-86db-63c64db9c2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_interconnection_load_time_series(data_input_dir = data_input_dir, \n",
    "                                         scenario = 'rcp85hotter_ssp5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782d3f1c-726f-4210-8bbb-f5dccfb27c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.9.15_std",
   "language": "python",
   "name": "py3.9.15_std"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
