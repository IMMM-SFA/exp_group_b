{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd40f4f4-5a80-40b7-94b3-aed136daa9d8",
   "metadata": {},
   "source": [
    "# Divergence: Analysis of Load Difference Time Series\n",
    "\n",
    "This notebook analyzes the time-series of the annual mean difference in load projections across pairs of IM3 scenarios. The analysis is initially done at the state level but could readily be expanded to work on balancing authorities as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27e597e9-538f-41d4-afa9-a268b10ab1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by importing the packages we need:\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "\n",
    "from glob import glob\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795f6fe5-0769-4298-887e-7c4cffc64a0d",
   "metadata": {},
   "source": [
    "## Set the Directory Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd887600-32da-46dc-b411-fa63c75f564f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Identify the data input and output directories:\n",
    "data_input_dir =  '/Users/burl878/Documents/IMMM/Data/TELL/Production_Runs/tell_data/outputs/tell_output/'\n",
    "data_output_dir =  '/Users/burl878/Documents/Code/code_repos/exp_group_b_test/analyses/data/'\n",
    "image_output_dir =  '/Users/burl878/Documents/Code/code_repos/exp_group_b_test/analyses/plots/load_difference_time_series/'\n",
    "\n",
    "# If the \"image_output_dir\" subdirectory doesn't exist then create it:\n",
    "if not os.path.exists(image_output_dir):\n",
    "   os.makedirs(image_output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f055d71-411a-4b85-b0b8-5fd3f9059796",
   "metadata": {},
   "source": [
    "## Process the Annual Mean Load Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b04f656-5fab-4fcb-86db-63c64db9c2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the annual mean load difference between 'scenario_one' and 'scenario_two':\n",
    "def process_load_difference_time_series(data_input_dir: str, process_states: bool, scenario_one: str, scenario_two: str):\n",
    "    #Initiate a counter to store the results:\n",
    "    counter = 0;\n",
    "    output_df = pd.DataFrame()\n",
    "    \n",
    "    # Loop over the years from 2020 to 2095 in 5 year increments:\n",
    "    for year in range(2020,2099,5):\n",
    "        if process_states == True:\n",
    "           # Read in the state-level time series for 'scenario_one':\n",
    "           df_one = pd.read_csv(data_input_dir + scenario_one + '/' + str(year) + '/TELL_State_Hourly_Load_Data_' + str(year) + '_Scaled_' + str(year) + '.csv')\n",
    "            \n",
    "           # Rename a few columns for simplicity:\n",
    "           df_one.rename(columns={'Scaled_TELL_State_Load_MWh': 'load_one', 'State_Name': 'name', 'Time_UTC': 'time'}, inplace=True)\n",
    "        \n",
    "           # Only keep the columns that are needed:\n",
    "           df_one = df_one[['name', 'time', 'load_one']].copy()\n",
    "        \n",
    "           # Read in the state-level time series for 'scenario_two':\n",
    "           df_two = pd.read_csv(data_input_dir + scenario_two + '/' + str(year) + '/TELL_State_Hourly_Load_Data_' + str(year) + '_Scaled_' + str(year) + '.csv')\n",
    "            \n",
    "           # Rename a few columns for simplicity:\n",
    "           df_two.rename(columns={'Scaled_TELL_State_Load_MWh': 'load_two', 'State_Name': 'name', 'Time_UTC': 'time'}, inplace=True)\n",
    "        \n",
    "           # Only keep the columns that are needed:\n",
    "           df_two = df_two[['name', 'time', 'load_two']].copy()\n",
    "           \n",
    "           # Merge the two time series together on state name and time:\n",
    "           merged_df = df_one.merge(df_two, on=['name', 'time'])\n",
    "            \n",
    "        if process_states == False:\n",
    "           # Read in the BA-level time series for 'scenario_one':\n",
    "           df_one = pd.read_csv(data_input_dir + scenario_one + '/' + str(year) + '/TELL_Balancing_Authority_Hourly_Load_Data_' + str(year) + '_Scaled_' + str(year) + '.csv')\n",
    "            \n",
    "           # Rename a few columns for simplicity:\n",
    "           df_one.rename(columns={'Scaled_TELL_BA_Load_MWh': 'load_one', 'BA_Code': 'name', 'Time_UTC': 'time'}, inplace=True)\n",
    "        \n",
    "           # Only keep the columns that are needed:\n",
    "           df_one = df_one[['name', 'time', 'load_one']].copy()    \n",
    "           \n",
    "           # Read in the BA-level time series for 'scenario_two':\n",
    "           df_two = pd.read_csv(data_input_dir + scenario_two + '/' + str(year) + '/TELL_Balancing_Authority_Hourly_Load_Data_' + str(year) + '_Scaled_' + str(year) + '.csv')\n",
    "            \n",
    "           # Rename a few columns for simplicity:\n",
    "           df_two.rename(columns={'Scaled_TELL_BA_Load_MWh': 'load_two', 'BA_Code': 'name', 'Time_UTC': 'time'}, inplace=True)\n",
    "        \n",
    "           # Only keep the columns that are needed:\n",
    "           df_two = df_two[['name', 'time', 'load_two']].copy()   \n",
    "           \n",
    "           # Merge the two time series together on BA name and time:\n",
    "           merged_df = df_one.merge(df_two, on=['name', 'time'])\n",
    "           \n",
    "        # Calculate the mean load between the two time-series as well as the absolute and relative load biases:\n",
    "        merged_df['mean_load'] =  merged_df[['load_one', 'load_two']].mean(axis=1)\n",
    "        merged_df['absolute_difference'] = abs(merged_df['load_one'] - merged_df['load_two'])\n",
    "        merged_df['relative_difference'] = 100*(merged_df['absolute_difference']/merged_df['mean_load']).round(4)\n",
    "       \n",
    "        # Make a list of all of the states or BAs in the 'merged_df':\n",
    "        entities = merged_df['name'].unique()\n",
    "         \n",
    "        # Loop over the entities and calculate the mean bias for each entity:\n",
    "        for i in range(len(entities)):\n",
    "            # Iterate the counter by one:\n",
    "            counter = counter + 1\n",
    "            \n",
    "            # Subset to just the data for the entity being processed:\n",
    "            subset_df = merged_df[merged_df['name'].isin([entities[i]])].copy()\n",
    "          \n",
    "            # Subset to the top 10% of all mean load hours:\n",
    "            peak_df = subset_df.nlargest(876,'mean_load')\n",
    "        \n",
    "            # Put the output in a new dataframe:\n",
    "            output_df.loc[counter, 'Year'] = str(year)\n",
    "            output_df.loc[counter, 'Name'] = entities[i]\n",
    "            output_df.loc[counter, 'Mean_Abs_Diff_MWh'] = subset_df['absolute_difference'].mean().round(2)\n",
    "            output_df.loc[counter, 'Mean_Rel_Diff_%'] = subset_df['relative_difference'].mean().round(2)\n",
    "            output_df.loc[counter, 'Peak_Abs_Diff_MWh'] = peak_df['absolute_difference'].mean().round(2)\n",
    "            output_df.loc[counter, 'Peak_Rel_Diff_%'] = peak_df['relative_difference'].mean().round(2)\n",
    "        \n",
    "            # Clean up and move to the next step in the loop:\n",
    "            del subset_df, peak_df\n",
    "    \n",
    "    # Generate the .csv output file name:\n",
    "    if process_states == True:\n",
    "       csv_output_filename = os.path.join(data_output_dir, 'State_Differences_' + scenario_one + '_Versus_' + scenario_two + '.csv')\n",
    "    if process_states == False:\n",
    "       csv_output_filename = os.path.join(data_output_dir, 'BA_Differences_' + scenario_one + '_Versus_' + scenario_two + '.csv')\n",
    "\n",
    "    # Write out the dataframe to a .csv file:\n",
    "    output_df.to_csv(csv_output_filename, sep=',', index=False)\n",
    "    \n",
    "    return output_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aeab7320-44c8-4afd-b81e-2865afc12ec3",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '/Users/burl878/Documents/Code/code_repos/exp_group_b_test/analyses/data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m gcm_div_d_one \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrcp85hotter_ssp5\u001b[39m\u001b[38;5;124m'\u001b[39m; gcm_div_d_two \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrcp85cooler_ssp5\u001b[39m\u001b[38;5;124m'\u001b[39m;\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Process the load time series for each pair of scenarios\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m output_df \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_load_difference_time_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_input_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_input_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mprocess_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mscenario_one\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mssp_div_a_one\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mscenario_two\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mssp_div_a_two\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m output_df\n",
      "Cell \u001b[0;32mIn [3], line 90\u001b[0m, in \u001b[0;36mprocess_load_difference_time_series\u001b[0;34m(data_input_dir, process_states, scenario_one, scenario_two)\u001b[0m\n\u001b[1;32m     87\u001b[0m    csv_output_filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_output_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBA_Differences_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m scenario_one \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_Versus_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m scenario_two \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Write out the dataframe to a .csv file:\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m \u001b[43moutput_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_output_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_df\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/py3.9.15_std/lib/python3.9/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/py3.9.15_std/lib/python3.9/site-packages/pandas/core/generic.py:3720\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3709\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[1;32m   3711\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3712\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m   3713\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3717\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[1;32m   3718\u001b[0m )\n\u001b[0;32m-> 3720\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3723\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3725\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3727\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3737\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/py3.9.15_std/lib/python3.9/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/py3.9.15_std/lib/python3.9/site-packages/pandas/io/formats/format.py:1189\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1168\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[1;32m   1171\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[1;32m   1172\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1187\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[1;32m   1188\u001b[0m )\n\u001b[0;32m-> 1189\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1192\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/py3.9.15_std/lib/python3.9/site-packages/pandas/io/formats/csvs.py:241\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    249\u001b[0m \n\u001b[1;32m    250\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[1;32m    252\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m    253\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[1;32m    259\u001b[0m     )\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/py3.9.15_std/lib/python3.9/site-packages/pandas/io/common.py:734\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[0;32m--> 734\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    738\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/envs/py3.9.15_std/lib/python3.9/site-packages/pandas/io/common.py:597\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    595\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: '/Users/burl878/Documents/Code/code_repos/exp_group_b_test/analyses/data'"
     ]
    }
   ],
   "source": [
    "# Set the groups of scenarios to plot:\n",
    "ssp_div_a_one = 'rcp45cooler_ssp5'; ssp_div_a_two = 'rcp45cooler_ssp3';\n",
    "ssp_div_b_one = 'rcp45hotter_ssp5'; ssp_div_b_two = 'rcp45hotter_ssp3';\n",
    "ssp_div_c_one = 'rcp85cooler_ssp5'; ssp_div_c_two = 'rcp85cooler_ssp3';\n",
    "ssp_div_d_one = 'rcp85hotter_ssp5'; ssp_div_d_two = 'rcp85hotter_ssp3';\n",
    "  \n",
    "rcp_div_a_one = 'rcp85cooler_ssp3'; rcp_div_a_two = 'rcp45cooler_ssp3'; \n",
    "rcp_div_b_one = 'rcp85hotter_ssp3'; rcp_div_b_two = 'rcp45hotter_ssp3'; \n",
    "rcp_div_c_one = 'rcp85cooler_ssp5'; rcp_div_c_two = 'rcp45cooler_ssp5'; \n",
    "rcp_div_d_one = 'rcp85hotter_ssp5'; rcp_div_d_two = 'rcp45hotter_ssp5';\n",
    "    \n",
    "gcm_div_a_one = 'rcp45hotter_ssp3'; gcm_div_a_two = 'rcp45cooler_ssp3'; \n",
    "gcm_div_b_one = 'rcp45hotter_ssp5'; gcm_div_b_two = 'rcp45cooler_ssp5'; \n",
    "gcm_div_c_one = 'rcp85hotter_ssp3'; gcm_div_c_two = 'rcp85cooler_ssp3'; \n",
    "gcm_div_d_one = 'rcp85hotter_ssp5'; gcm_div_d_two = 'rcp85cooler_ssp5';\n",
    "\n",
    "# Process the load time series for each pair of scenarios\n",
    "output_df = process_load_difference_time_series(data_input_dir = data_input_dir, \n",
    "                                                process_states = True, \n",
    "                                                scenario_one = ssp_div_a_one, \n",
    "                                                scenario_two = ssp_div_a_two)\n",
    "\n",
    "output_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b635da7-f864-4bc6-94bc-537a20f01f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the point of divergence across all scenario combinations\n",
    "def process_point_of_divergence(data_input_dir: str, process_states: bool, threshold: int):\n",
    "    #Initiate a counter to store the results:\n",
    "    counter = 0;\n",
    "    output_df = pd.DataFrame()\n",
    "    \n",
    "    # Create a list of all files in the input directory:\n",
    "    if process_states == True:\n",
    "       list_of_files = glob(f'{data_input_dir}State_Differences*.csv')\n",
    "    if process_states == False:\n",
    "       list_of_files = glob(f'{data_input_dir}BA_Differences*.csv')  \n",
    "    \n",
    "    # Loop over the list of files and compute the point of divergence for each entity:\n",
    "    for file in range(len(list_of_files)):\n",
    "        # Read in the .csv file:\n",
    "        df = pd.read_csv(list_of_files[file])\n",
    "    \n",
    "        # Extract the scenarios from the filename:\n",
    "        if process_states == True:\n",
    "           filename = str(os.path.splitext(os.path.basename(list_of_files[file]))[0]).replace('_Versus', '_').replace('State_Differences_', '')\n",
    "        if process_states == False:\n",
    "           filename = str(os.path.splitext(os.path.basename(list_of_files[file]))[0]).replace('_Versus', '_').replace('BA_Differences_', '')\n",
    "        scenario_one = filename.split('__')[0]\n",
    "        scenario_two = filename.split('__')[1]\n",
    "    \n",
    "        # Make a list of all of the states or BAs in the 'merged_df':\n",
    "        entities = df['Name'].unique()\n",
    "         \n",
    "        # Loop over the entities and calculate the point of divergence for each entity:\n",
    "        for i in range(len(entities)):\n",
    "            # Iterate the counter by one:\n",
    "            counter = counter + 1\n",
    "            \n",
    "            # Subset to just the data for the entity being processed:\n",
    "            subset_df = df[df['Name'].isin([entities[i]])].copy()\n",
    "            \n",
    "            # Initiate empty break year and break values:\n",
    "            mean_break_year = np.nan\n",
    "            mean_break_value = np.nan\n",
    "            peak_break_year = np.nan\n",
    "            peak_break_value = np.nan\n",
    "                \n",
    "            # Loop over the rows of the dataframe and check if the mean value in the row exceeds the threshold value:\n",
    "            for row in range(len(subset_df)-1):\n",
    "                # If the difference in the year and the following year exceeds the threshold value then stop the loop and output the results:\n",
    "                if (subset_df['Mean_Rel_Diff_%'].iloc[row] >= threshold) & (subset_df['Mean_Rel_Diff_%'].iloc[row+1] >= threshold):\n",
    "                    mean_break_year = subset_df['Year'].iloc[row]\n",
    "                    mean_break_value = subset_df['Mean_Rel_Diff_%'].iloc[row]\n",
    "                    break\n",
    "            \n",
    "            # Loop over the rows of the dataframe and check if the peak value in the row exceeds the threshold value:\n",
    "            for row in range(len(subset_df)-1):\n",
    "                # If the difference in the year and the following year exceeds the threshold value then stop the loop and output the results:\n",
    "                if (subset_df['Peak_Rel_Diff_%'].iloc[row] >= threshold) & (subset_df['Peak_Rel_Diff_%'].iloc[row+1] >= threshold):\n",
    "                    peak_break_year = subset_df['Year'].iloc[row]\n",
    "                    peak_break_value = subset_df['Peak_Rel_Diff_%'].iloc[row]\n",
    "                    break\n",
    "\n",
    "            # Put the output in a new dataframe:\n",
    "            output_df.loc[counter, 'Name'] = entities[i]\n",
    "            output_df.loc[counter, 'Scenario_One'] = scenario_one\n",
    "            output_df.loc[counter, 'Scenario_Two'] = scenario_two\n",
    "            output_df.loc[counter, 'Mean_Break_Year'] = mean_break_year\n",
    "            output_df.loc[counter, 'Mean_Break_Value_%'] = mean_break_value\n",
    "            output_df.loc[counter, 'Peak_Break_Year'] = peak_break_year\n",
    "            output_df.loc[counter, 'Peak_Break_Value_%'] = peak_break_value\n",
    "            \n",
    "            # Clean up:\n",
    "            del subset_df, row, mean_break_year, mean_break_value, peak_break_year, peak_break_value\n",
    "        \n",
    "        # Clean up:\n",
    "        del df, filename, scenario_one, scenario_two, i, entities\n",
    "    \n",
    "    # Generate the .csv output file name:\n",
    "    if process_states == True:\n",
    "       csv_output_filename = os.path.join(data_output_dir, 'State_Divergence_Years_' + str(threshold) + '.csv')\n",
    "    if process_states == False:\n",
    "       csv_output_filename = os.path.join(data_output_dir, 'BA_Divergence_Years_' + str(threshold) + '.csv')\n",
    "\n",
    "    # Write out the dataframe to a .csv file:\n",
    "    output_df.to_csv(csv_output_filename, sep=',', index=False)\n",
    "    \n",
    "    return output_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4705e6c9-3fbe-4cb0-b853-b49a8fe67f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = process_point_of_divergence(data_input_dir = data_output_dir, \n",
    "                                        process_states = True, \n",
    "                                        threshold = 10)\n",
    "\n",
    "output_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc65f89-cd8f-4a78-9cb9-c93e4f6b91ac",
   "metadata": {},
   "source": [
    "## Make the Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98bab0d-9647-49f7-b6c5-e44a510b826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot the time series for a single entity:\n",
    "def plot_entity_point_of_divergence(data_input_dir: str, entity: str, process_states: bool, image_output_dir: str, image_resolution: int, save_images=False):\n",
    "    \n",
    "    # Set the groups of scenarios to plot:\n",
    "    ssp_div_a_one = 'rcp45cooler_ssp5'; ssp_div_a_two = 'rcp45cooler_ssp3';\n",
    "    ssp_div_b_one = 'rcp45hotter_ssp5'; ssp_div_b_two = 'rcp45hotter_ssp3';\n",
    "    ssp_div_c_one = 'rcp85cooler_ssp5'; ssp_div_c_two = 'rcp85cooler_ssp3';\n",
    "    ssp_div_d_one = 'rcp85hotter_ssp5'; ssp_div_d_two = 'rcp85hotter_ssp3';\n",
    "    \n",
    "    rcp_div_a_one = 'rcp85cooler_ssp3'; rcp_div_a_two = 'rcp45cooler_ssp3'; \n",
    "    rcp_div_b_one = 'rcp85hotter_ssp3'; rcp_div_b_two = 'rcp45hotter_ssp3'; \n",
    "    rcp_div_c_one = 'rcp85cooler_ssp5'; rcp_div_c_two = 'rcp45cooler_ssp5'; \n",
    "    rcp_div_d_one = 'rcp85hotter_ssp5'; rcp_div_d_two = 'rcp45hotter_ssp5';\n",
    "    \n",
    "    gcm_div_a_one = 'rcp45hotter_ssp3'; gcm_div_a_two = 'rcp45cooler_ssp3'; \n",
    "    gcm_div_b_one = 'rcp45hotter_ssp5'; gcm_div_b_two = 'rcp45cooler_ssp5'; \n",
    "    gcm_div_c_one = 'rcp85hotter_ssp3'; gcm_div_c_two = 'rcp85cooler_ssp3'; \n",
    "    gcm_div_d_one = 'rcp85hotter_ssp5'; gcm_div_d_two = 'rcp85cooler_ssp5';\n",
    "    \n",
    "    # Load in the time series:\n",
    "    if process_states == True:\n",
    "       ssp_div_a_df = pd.read_csv(data_input_dir + 'State_Differences_' + ssp_div_a_one + '_Versus_' + ssp_div_a_two + '.csv'); ssp_div_a_df = ssp_div_a_df[ssp_div_a_df['Name'].isin([entity])]\n",
    "       ssp_div_b_df = pd.read_csv(data_input_dir + 'State_Differences_' + ssp_div_b_one + '_Versus_' + ssp_div_b_two + '.csv'); ssp_div_b_df = ssp_div_b_df[ssp_div_b_df['Name'].isin([entity])]\n",
    "       ssp_div_c_df = pd.read_csv(data_input_dir + 'State_Differences_' + ssp_div_c_one + '_Versus_' + ssp_div_c_two + '.csv'); ssp_div_c_df = ssp_div_c_df[ssp_div_c_df['Name'].isin([entity])]\n",
    "       ssp_div_d_df = pd.read_csv(data_input_dir + 'State_Differences_' + ssp_div_d_one + '_Versus_' + ssp_div_d_two + '.csv'); ssp_div_d_df = ssp_div_d_df[ssp_div_d_df['Name'].isin([entity])] \n",
    "        \n",
    "       rcp_div_a_df = pd.read_csv(data_input_dir + 'State_Differences_' + rcp_div_a_one + '_Versus_' + rcp_div_a_two + '.csv'); rcp_div_a_df = rcp_div_a_df[rcp_div_a_df['Name'].isin([entity])]\n",
    "       rcp_div_b_df = pd.read_csv(data_input_dir + 'State_Differences_' + rcp_div_b_one + '_Versus_' + rcp_div_b_two + '.csv'); rcp_div_b_df = rcp_div_b_df[rcp_div_b_df['Name'].isin([entity])]\n",
    "       rcp_div_c_df = pd.read_csv(data_input_dir + 'State_Differences_' + rcp_div_c_one + '_Versus_' + rcp_div_c_two + '.csv'); rcp_div_c_df = rcp_div_c_df[rcp_div_c_df['Name'].isin([entity])]\n",
    "       rcp_div_d_df = pd.read_csv(data_input_dir + 'State_Differences_' + rcp_div_d_one + '_Versus_' + rcp_div_d_two + '.csv'); rcp_div_d_df = rcp_div_d_df[rcp_div_d_df['Name'].isin([entity])] \n",
    "    \n",
    "       gcm_div_a_df = pd.read_csv(data_input_dir + 'State_Differences_' + gcm_div_a_one + '_Versus_' + gcm_div_a_two + '.csv'); gcm_div_a_df = gcm_div_a_df[gcm_div_a_df['Name'].isin([entity])]\n",
    "       gcm_div_b_df = pd.read_csv(data_input_dir + 'State_Differences_' + gcm_div_b_one + '_Versus_' + gcm_div_b_two + '.csv'); gcm_div_b_df = gcm_div_b_df[gcm_div_b_df['Name'].isin([entity])]\n",
    "       gcm_div_c_df = pd.read_csv(data_input_dir + 'State_Differences_' + gcm_div_c_one + '_Versus_' + gcm_div_c_two + '.csv'); gcm_div_c_df = gcm_div_c_df[gcm_div_c_df['Name'].isin([entity])]\n",
    "       gcm_div_d_df = pd.read_csv(data_input_dir + 'State_Differences_' + gcm_div_d_one + '_Versus_' + gcm_div_d_two + '.csv'); gcm_div_d_df = gcm_div_d_df[gcm_div_d_df['Name'].isin([entity])] \n",
    "    \n",
    "    if process_states == False:\n",
    "       ssp_div_a_df = pd.read_csv(data_input_dir + 'BA_Differences_' + ssp_div_a_one + '_Versus_' + ssp_div_a_two + '.csv'); ssp_div_a_df = ssp_div_a_df[ssp_div_a_df['Name'].isin([entity])]\n",
    "       ssp_div_b_df = pd.read_csv(data_input_dir + 'BA_Differences_' + ssp_div_b_one + '_Versus_' + ssp_div_b_two + '.csv'); ssp_div_b_df = ssp_div_b_df[ssp_div_b_df['Name'].isin([entity])]\n",
    "       ssp_div_c_df = pd.read_csv(data_input_dir + 'BA_Differences_' + ssp_div_c_one + '_Versus_' + ssp_div_c_two + '.csv'); ssp_div_c_df = ssp_div_c_df[ssp_div_c_df['Name'].isin([entity])]\n",
    "       ssp_div_d_df = pd.read_csv(data_input_dir + 'BA_Differences_' + ssp_div_d_one + '_Versus_' + ssp_div_d_two + '.csv'); ssp_div_d_df = ssp_div_d_df[ssp_div_d_df['Name'].isin([entity])] \n",
    "       \n",
    "       rcp_div_a_df = pd.read_csv(data_input_dir + 'BA_Differences_' + rcp_div_a_one + '_Versus_' + rcp_div_a_two + '.csv'); rcp_div_a_df = rcp_div_a_df[rcp_div_a_df['Name'].isin([entity])]\n",
    "       rcp_div_b_df = pd.read_csv(data_input_dir + 'BA_Differences_' + rcp_div_b_one + '_Versus_' + rcp_div_b_two + '.csv'); rcp_div_b_df = rcp_div_b_df[rcp_div_b_df['Name'].isin([entity])]\n",
    "       rcp_div_c_df = pd.read_csv(data_input_dir + 'BA_Differences_' + rcp_div_c_one + '_Versus_' + rcp_div_c_two + '.csv'); rcp_div_c_df = rcp_div_c_df[rcp_div_c_df['Name'].isin([entity])]\n",
    "       rcp_div_d_df = pd.read_csv(data_input_dir + 'BA_Differences_' + rcp_div_d_one + '_Versus_' + rcp_div_d_two + '.csv'); rcp_div_d_df = rcp_div_d_df[rcp_div_d_df['Name'].isin([entity])] \n",
    "     \n",
    "       gcm_div_a_df = pd.read_csv(data_input_dir + 'BA_Differences_' + gcm_div_a_one + '_Versus_' + gcm_div_a_two + '.csv'); gcm_div_a_df = gcm_div_a_df[gcm_div_a_df['Name'].isin([entity])]\n",
    "       gcm_div_b_df = pd.read_csv(data_input_dir + 'BA_Differences_' + gcm_div_b_one + '_Versus_' + gcm_div_b_two + '.csv'); gcm_div_b_df = gcm_div_b_df[gcm_div_b_df['Name'].isin([entity])]\n",
    "       gcm_div_c_df = pd.read_csv(data_input_dir + 'BA_Differences_' + gcm_div_c_one + '_Versus_' + gcm_div_c_two + '.csv'); gcm_div_c_df = gcm_div_c_df[gcm_div_c_df['Name'].isin([entity])]\n",
    "       gcm_div_d_df = pd.read_csv(data_input_dir + 'BA_Differences_' + gcm_div_d_one + '_Versus_' + gcm_div_d_two + '.csv'); gcm_div_d_df = gcm_div_d_df[gcm_div_d_df['Name'].isin([entity])]\n",
    "    \n",
    "    # Make the plot:\n",
    "    plt.figure(figsize=(24, 15))\n",
    "    plt.rcParams['font.size'] = 16\n",
    "    \n",
    "    plt.subplot(221)\n",
    "    plt.plot(-5, 0, color='k', linestyle='-', label='Mean Load Difference', linewidth=2)\n",
    "    plt.plot(-5, 0, color='k', linestyle=':', label='Peak Load Difference', linewidth=2)\n",
    "    plt.plot(ssp_div_a_df['Year'], ssp_div_a_df['Mean_Rel_Diff_%'], color='b', linestyle='-', label=(ssp_div_a_one + ' vs ' + ssp_div_a_two), linewidth=2)\n",
    "    plt.plot(ssp_div_b_df['Year'], ssp_div_b_df['Mean_Rel_Diff_%'], color='cyan', linestyle='-', label=(ssp_div_b_one + ' vs ' + ssp_div_b_two), linewidth=2)\n",
    "    plt.plot(ssp_div_c_df['Year'], ssp_div_c_df['Mean_Rel_Diff_%'], color='orange', linestyle='-', label=(ssp_div_c_one + ' vs ' + ssp_div_c_two), linewidth=2)\n",
    "    plt.plot(ssp_div_d_df['Year'], ssp_div_d_df['Mean_Rel_Diff_%'], color='r', linestyle='-', label=(ssp_div_d_one + ' vs ' + ssp_div_d_two), linewidth=2)\n",
    "    plt.plot(ssp_div_a_df['Year'], ssp_div_a_df['Peak_Rel_Diff_%'], color='b', linestyle=':', linewidth=2)\n",
    "    plt.plot(ssp_div_b_df['Year'], ssp_div_b_df['Peak_Rel_Diff_%'], color='cyan', linestyle=':', linewidth=2)\n",
    "    plt.plot(ssp_div_c_df['Year'], ssp_div_c_df['Peak_Rel_Diff_%'], color='orange', linestyle=':', linewidth=2)\n",
    "    plt.plot(ssp_div_d_df['Year'], ssp_div_d_df['Peak_Rel_Diff_%'], color='r', linestyle=':', linewidth=2)\n",
    "    plt.xlim([2020, 2095]); plt.ylim(bottom=0)\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='upper left', prop={'size': 14})\n",
    "    plt.xlabel('Year'); plt.ylabel('Annual Mean Hourly Load Difference [%]')\n",
    "    plt.title(('Socioeconomic Scenario Divergence [SSP3 vs SSP5]: ' + entity))\n",
    "    \n",
    "    plt.subplot(222)\n",
    "    plt.plot(-5, 0, color='k', linestyle='-', label='Mean Load Difference', linewidth=2)\n",
    "    plt.plot(-5, 0, color='k', linestyle=':', label='Peak Load Difference', linewidth=2)\n",
    "    plt.plot(rcp_div_a_df['Year'], rcp_div_a_df['Mean_Rel_Diff_%'], color='forestgreen', linestyle='-', label=(rcp_div_a_one + ' vs ' + rcp_div_a_two), linewidth=2)\n",
    "    plt.plot(rcp_div_b_df['Year'], rcp_div_b_df['Mean_Rel_Diff_%'], color='blueviolet', linestyle='-', label=(rcp_div_b_one + ' vs ' + rcp_div_b_two), linewidth=2)\n",
    "    plt.plot(rcp_div_c_df['Year'], rcp_div_c_df['Mean_Rel_Diff_%'], color='goldenrod', linestyle='-', label=(rcp_div_c_one + ' vs ' + rcp_div_c_two), linewidth=2)\n",
    "    plt.plot(rcp_div_d_df['Year'], rcp_div_d_df['Mean_Rel_Diff_%'], color='lightcoral', linestyle='-', label=(rcp_div_d_one + ' vs ' + rcp_div_d_two), linewidth=2)\n",
    "    plt.plot(rcp_div_a_df['Year'], rcp_div_a_df['Peak_Rel_Diff_%'], color='forestgreen', linestyle=':', linewidth=2)\n",
    "    plt.plot(rcp_div_b_df['Year'], rcp_div_b_df['Peak_Rel_Diff_%'], color='blueviolet', linestyle=':', linewidth=2)\n",
    "    plt.plot(rcp_div_c_df['Year'], rcp_div_c_df['Peak_Rel_Diff_%'], color='goldenrod', linestyle=':', linewidth=2)\n",
    "    plt.plot(rcp_div_d_df['Year'], rcp_div_d_df['Peak_Rel_Diff_%'], color='lightcoral', linestyle=':', linewidth=2)\n",
    "    plt.xlim([2020, 2095]); plt.ylim(bottom=0)\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='upper left', prop={'size': 14})\n",
    "    plt.xlabel('Year'); plt.ylabel('Annual Mean Hourly Load Difference [%]')\n",
    "    plt.title(('Climate Scenario Divergence [RCP 4.5 vs RCP 8.5]: ' + entity))\n",
    "    \n",
    "    plt.subplot(223)\n",
    "    plt.plot(-5, 0, color='k', linestyle='-', label='Mean Load Difference', linewidth=2)\n",
    "    plt.plot(-5, 0, color='k', linestyle=':', label='Peak Load Difference', linewidth=2)\n",
    "    plt.plot(gcm_div_a_df['Year'], gcm_div_a_df['Mean_Rel_Diff_%'], color='brown', linestyle='-', label=(gcm_div_a_one + ' vs ' + gcm_div_a_two), linewidth=2)\n",
    "    plt.plot(gcm_div_b_df['Year'], gcm_div_b_df['Mean_Rel_Diff_%'], color='gold', linestyle='-', label=(gcm_div_b_one + ' vs ' + gcm_div_b_two), linewidth=2)\n",
    "    plt.plot(gcm_div_c_df['Year'], gcm_div_c_df['Mean_Rel_Diff_%'], color='skyblue', linestyle='-', label=(gcm_div_c_one + ' vs ' + gcm_div_c_two), linewidth=2)\n",
    "    plt.plot(gcm_div_d_df['Year'], gcm_div_d_df['Mean_Rel_Diff_%'], color='magenta', linestyle='-', label=(gcm_div_d_one + ' vs ' + gcm_div_d_two), linewidth=2)\n",
    "    plt.plot(gcm_div_a_df['Year'], gcm_div_a_df['Peak_Rel_Diff_%'], color='brown', linestyle=':', linewidth=2)\n",
    "    plt.plot(gcm_div_b_df['Year'], gcm_div_b_df['Peak_Rel_Diff_%'], color='gold', linestyle=':', linewidth=2)\n",
    "    plt.plot(gcm_div_c_df['Year'], gcm_div_c_df['Peak_Rel_Diff_%'], color='skyblue', linestyle=':', linewidth=2)\n",
    "    plt.plot(gcm_div_d_df['Year'], gcm_div_d_df['Peak_Rel_Diff_%'], color='magenta', linestyle=':', linewidth=2)\n",
    "    plt.xlim([2020, 2095]); plt.ylim(bottom=0)\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='upper left', prop={'size': 14})\n",
    "    plt.xlabel('Year'); plt.ylabel('Annual Mean Hourly Load Difference [%]')\n",
    "    plt.title(('Climate Model Divergence [Hotter vs Colder Models]: ' + entity))\n",
    "    \n",
    "    # If the \"save_images\" flag is set to true then save the plot to a .png file:\n",
    "    if save_images == True:\n",
    "       if process_states == True:\n",
    "          filename = ('State_' + entity.replace(\" \", \"_\") + '_Load_Divergence.png')\n",
    "       if process_states == False:\n",
    "          filename = ('BA_' + entity.replace(\" \", \"_\") + '_Load_Divergence.png')\n",
    "       plt.savefig(os.path.join(image_output_dir, filename), dpi=image_resolution, bbox_inches='tight', facecolor='white')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a10c6fa-181c-4453-ac03-82b94470ea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_entity_point_of_divergence(data_input_dir = data_output_dir, \n",
    "                                entity = 'Arizona', \n",
    "                                process_states = True,\n",
    "                                image_output_dir = image_output_dir, \n",
    "                                image_resolution = 300, \n",
    "                                save_images = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b928702f-7683-4454-8e68-a02de25e31a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot the change point distribution for all states or BAs:\n",
    "def plot_group_point_of_divergence(data_input_dir: str, threshold: int, process_states: bool, image_output_dir: str, image_resolution: int, save_images=False):\n",
    "    \n",
    "    # Set the groups of scenarios to plot:\n",
    "    ssp_div_a_one = 'rcp45cooler_ssp5'; ssp_div_a_two = 'rcp45cooler_ssp3';\n",
    "    ssp_div_b_one = 'rcp45hotter_ssp5'; ssp_div_b_two = 'rcp45hotter_ssp3';\n",
    "    ssp_div_c_one = 'rcp85cooler_ssp5'; ssp_div_c_two = 'rcp85cooler_ssp3';\n",
    "    ssp_div_d_one = 'rcp85hotter_ssp5'; ssp_div_d_two = 'rcp85hotter_ssp3';\n",
    "    \n",
    "    rcp_div_a_one = 'rcp85cooler_ssp3'; rcp_div_a_two = 'rcp45cooler_ssp3'; \n",
    "    rcp_div_b_one = 'rcp85hotter_ssp3'; rcp_div_b_two = 'rcp45hotter_ssp3'; \n",
    "    rcp_div_c_one = 'rcp85cooler_ssp5'; rcp_div_c_two = 'rcp45cooler_ssp5'; \n",
    "    rcp_div_d_one = 'rcp85hotter_ssp5'; rcp_div_d_two = 'rcp45hotter_ssp5';\n",
    "    \n",
    "    gcm_div_a_one = 'rcp45hotter_ssp3'; gcm_div_a_two = 'rcp45cooler_ssp3'; \n",
    "    gcm_div_b_one = 'rcp45hotter_ssp5'; gcm_div_b_two = 'rcp45cooler_ssp5'; \n",
    "    gcm_div_c_one = 'rcp85hotter_ssp3'; gcm_div_c_two = 'rcp85cooler_ssp3'; \n",
    "    gcm_div_d_one = 'rcp85hotter_ssp5'; gcm_div_d_two = 'rcp85cooler_ssp5';\n",
    "    \n",
    "    # Load in the time series:\n",
    "    if process_states == True:\n",
    "       div_df = pd.read_csv(data_input_dir + 'State_Divergence_Years_' + str(threshold) + '.csv') \n",
    "    if process_states == False:\n",
    "       div_df = pd.read_csv(data_input_dir + 'BA_Divergence_Years_' + str(threshold) + '.csv') \n",
    "    \n",
    "    counter = 0; ssp_div_a = pd.DataFrame(); subset_df = div_df.loc[(div_df['Scenario_One'] == ssp_div_a_one) & (div_df['Scenario_Two'] == ssp_div_a_two)]\n",
    "    for year in range(2020,2099,5):\n",
    "        counter = counter + 1;\n",
    "        ssp_div_a.loc[counter, 'Year'] = year;\n",
    "        ssp_div_a.loc[counter, 'Mean_Breaks'] = (subset_df.loc[subset_df['Mean_Break_Year'] <= year]).shape[0]\n",
    "        ssp_div_a.loc[counter, 'Peak_Breaks'] = (subset_df.loc[subset_df['Peak_Break_Year'] <= year]).shape[0]\n",
    "    del counter, subset_df, year\n",
    "    \n",
    "    counter = 0; ssp_div_b = pd.DataFrame(); subset_df = div_df.loc[(div_df['Scenario_One'] == ssp_div_b_one) & (div_df['Scenario_Two'] == ssp_div_b_two)]\n",
    "    for year in range(2020,2099,5):\n",
    "        counter = counter + 1;\n",
    "        ssp_div_b.loc[counter, 'Year'] = year;\n",
    "        ssp_div_b.loc[counter, 'Mean_Breaks'] = (subset_df.loc[subset_df['Mean_Break_Year'] <= year]).shape[0]\n",
    "        ssp_div_b.loc[counter, 'Peak_Breaks'] = (subset_df.loc[subset_df['Peak_Break_Year'] <= year]).shape[0]\n",
    "    del counter, subset_df, year\n",
    "       \n",
    "    counter = 0; ssp_div_c = pd.DataFrame(); subset_df = div_df.loc[(div_df['Scenario_One'] == ssp_div_c_one) & (div_df['Scenario_Two'] == ssp_div_c_two)]\n",
    "    for year in range(2020,2099,5):\n",
    "        counter = counter + 1;\n",
    "        ssp_div_c.loc[counter, 'Year'] = year;\n",
    "        ssp_div_c.loc[counter, 'Mean_Breaks'] = (subset_df.loc[subset_df['Mean_Break_Year'] <= year]).shape[0]\n",
    "        ssp_div_c.loc[counter, 'Peak_Breaks'] = (subset_df.loc[subset_df['Peak_Break_Year'] <= year]).shape[0]\n",
    "    del counter, subset_df, year\n",
    "    \n",
    "    counter = 0; ssp_div_d = pd.DataFrame(); subset_df = div_df.loc[(div_df['Scenario_One'] == ssp_div_d_one) & (div_df['Scenario_Two'] == ssp_div_d_two)]\n",
    "    for year in range(2020,2099,5):\n",
    "        counter = counter + 1;\n",
    "        ssp_div_d.loc[counter, 'Year'] = year;\n",
    "        ssp_div_d.loc[counter, 'Mean_Breaks'] = (subset_df.loc[subset_df['Mean_Break_Year'] <= year]).shape[0]\n",
    "        ssp_div_d.loc[counter, 'Peak_Breaks'] = (subset_df.loc[subset_df['Peak_Break_Year'] <= year]).shape[0]\n",
    "    del counter, subset_df, year\n",
    "\n",
    "    counter = 0; rcp_div_a = pd.DataFrame(); subset_df = div_df.loc[(div_df['Scenario_One'] == rcp_div_a_one) & (div_df['Scenario_Two'] == rcp_div_a_two)]\n",
    "    for year in range(2020,2099,5):\n",
    "        counter = counter + 1;\n",
    "        rcp_div_a.loc[counter, 'Year'] = year;\n",
    "        rcp_div_a.loc[counter, 'Mean_Breaks'] = (subset_df.loc[subset_df['Mean_Break_Year'] <= year]).shape[0]\n",
    "        rcp_div_a.loc[counter, 'Peak_Breaks'] = (subset_df.loc[subset_df['Peak_Break_Year'] <= year]).shape[0]\n",
    "    del counter, subset_df, year\n",
    "    \n",
    "    counter = 0; rcp_div_b = pd.DataFrame(); subset_df = div_df.loc[(div_df['Scenario_One'] == rcp_div_b_one) & (div_df['Scenario_Two'] == rcp_div_b_two)]\n",
    "    for year in range(2020,2099,5):\n",
    "        counter = counter + 1;\n",
    "        rcp_div_b.loc[counter, 'Year'] = year;\n",
    "        rcp_div_b.loc[counter, 'Mean_Breaks'] = (subset_df.loc[subset_df['Mean_Break_Year'] <= year]).shape[0]\n",
    "        rcp_div_b.loc[counter, 'Peak_Breaks'] = (subset_df.loc[subset_df['Peak_Break_Year'] <= year]).shape[0]\n",
    "    del counter, subset_df, year\n",
    "    \n",
    "    counter = 0; rcp_div_c = pd.DataFrame(); subset_df = div_df.loc[(div_df['Scenario_One'] == rcp_div_c_one) & (div_df['Scenario_Two'] == rcp_div_c_two)]\n",
    "    for year in range(2020,2099,5):\n",
    "        counter = counter + 1;\n",
    "        rcp_div_c.loc[counter, 'Year'] = year;\n",
    "        rcp_div_c.loc[counter, 'Mean_Breaks'] = (subset_df.loc[subset_df['Mean_Break_Year'] <= year]).shape[0]\n",
    "        rcp_div_c.loc[counter, 'Peak_Breaks'] = (subset_df.loc[subset_df['Peak_Break_Year'] <= year]).shape[0]\n",
    "    del counter, subset_df, year\n",
    "    \n",
    "    counter = 0; rcp_div_d = pd.DataFrame(); subset_df = div_df.loc[(div_df['Scenario_One'] == rcp_div_d_one) & (div_df['Scenario_Two'] == rcp_div_d_two)]\n",
    "    for year in range(2020,2099,5):\n",
    "        counter = counter + 1;\n",
    "        rcp_div_d.loc[counter, 'Year'] = year;\n",
    "        rcp_div_d.loc[counter, 'Mean_Breaks'] = (subset_df.loc[subset_df['Mean_Break_Year'] <= year]).shape[0]\n",
    "        rcp_div_d.loc[counter, 'Peak_Breaks'] = (subset_df.loc[subset_df['Peak_Break_Year'] <= year]).shape[0]\n",
    "    del counter, subset_df, year\n",
    "    \n",
    "    counter = 0; gcm_div_a = pd.DataFrame(); subset_df = div_df.loc[(div_df['Scenario_One'] == gcm_div_a_one) & (div_df['Scenario_Two'] == gcm_div_a_two)]\n",
    "    for year in range(2020,2099,5):\n",
    "        counter = counter + 1;\n",
    "        gcm_div_a.loc[counter, 'Year'] = year;\n",
    "        gcm_div_a.loc[counter, 'Mean_Breaks'] = (subset_df.loc[subset_df['Mean_Break_Year'] <= year]).shape[0]\n",
    "        gcm_div_a.loc[counter, 'Peak_Breaks'] = (subset_df.loc[subset_df['Peak_Break_Year'] <= year]).shape[0]\n",
    "    del counter, subset_df, year\n",
    "    \n",
    "    counter = 0; gcm_div_b = pd.DataFrame(); subset_df = div_df.loc[(div_df['Scenario_One'] == gcm_div_b_one) & (div_df['Scenario_Two'] == gcm_div_b_two)]\n",
    "    for year in range(2020,2099,5):\n",
    "        counter = counter + 1;\n",
    "        gcm_div_b.loc[counter, 'Year'] = year;\n",
    "        gcm_div_b.loc[counter, 'Mean_Breaks'] = (subset_df.loc[subset_df['Mean_Break_Year'] <= year]).shape[0]\n",
    "        gcm_div_b.loc[counter, 'Peak_Breaks'] = (subset_df.loc[subset_df['Peak_Break_Year'] <= year]).shape[0]\n",
    "    del counter, subset_df, year\n",
    "    \n",
    "    counter = 0; gcm_div_c = pd.DataFrame(); subset_df = div_df.loc[(div_df['Scenario_One'] == gcm_div_c_one) & (div_df['Scenario_Two'] == gcm_div_c_two)]\n",
    "    for year in range(2020,2099,5):\n",
    "        counter = counter + 1;\n",
    "        gcm_div_c.loc[counter, 'Year'] = year;\n",
    "        gcm_div_c.loc[counter, 'Mean_Breaks'] = (subset_df.loc[subset_df['Mean_Break_Year'] <= year]).shape[0]\n",
    "        gcm_div_c.loc[counter, 'Peak_Breaks'] = (subset_df.loc[subset_df['Peak_Break_Year'] <= year]).shape[0]\n",
    "    del counter, subset_df, year\n",
    "    \n",
    "    counter = 0; gcm_div_d = pd.DataFrame(); subset_df = div_df.loc[(div_df['Scenario_One'] == gcm_div_d_one) & (div_df['Scenario_Two'] == gcm_div_d_two)]\n",
    "    for year in range(2020,2099,5):\n",
    "        counter = counter + 1;\n",
    "        gcm_div_d.loc[counter, 'Year'] = year;\n",
    "        gcm_div_d.loc[counter, 'Mean_Breaks'] = (subset_df.loc[subset_df['Mean_Break_Year'] <= year]).shape[0]\n",
    "        gcm_div_d.loc[counter, 'Peak_Breaks'] = (subset_df.loc[subset_df['Peak_Break_Year'] <= year]).shape[0]\n",
    "    del counter, subset_df, year\n",
    "    \n",
    "    # Make the plot:\n",
    "    plt.figure(figsize=(24, 15))\n",
    "    plt.rcParams['font.size'] = 14\n",
    "    \n",
    "    plt.subplot(221)\n",
    "    plt.plot(-5, 0, color='k', linestyle='-', label='Mean Load', linewidth=2)\n",
    "    plt.plot(-5, 0, color='k', linestyle=':', label='Peak Load', linewidth=2)\n",
    "    plt.plot(ssp_div_a['Year'], ssp_div_a['Mean_Breaks'], color='b', linestyle='-', label=(ssp_div_a_one + ' vs ' + ssp_div_a_two), linewidth=2)\n",
    "    plt.plot(ssp_div_b['Year'], ssp_div_b['Mean_Breaks'], color='cyan', linestyle='-', label=(ssp_div_b_one + ' vs ' + ssp_div_b_two), linewidth=2)\n",
    "    plt.plot(ssp_div_c['Year'], ssp_div_c['Mean_Breaks'], color='orange', linestyle='-', label=(ssp_div_c_one + ' vs ' + ssp_div_c_two), linewidth=2)\n",
    "    plt.plot(ssp_div_d['Year'], ssp_div_d['Mean_Breaks'], color='r', linestyle='-', label=(ssp_div_d_one + ' vs ' + ssp_div_d_two), linewidth=2)\n",
    "    plt.plot(ssp_div_a['Year'], ssp_div_a['Peak_Breaks'], color='b', linestyle=':', linewidth=2)\n",
    "    plt.plot(ssp_div_b['Year'], ssp_div_b['Peak_Breaks'], color='cyan', linestyle=':', linewidth=2)\n",
    "    plt.plot(ssp_div_c['Year'], ssp_div_c['Peak_Breaks'], color='orange', linestyle=':', linewidth=2)\n",
    "    plt.plot(ssp_div_d['Year'], ssp_div_d['Peak_Breaks'], color='r', linestyle=':', linewidth=2)\n",
    "    plt.xlim([2020, 2095]);\n",
    "    if process_states == True:\n",
    "       plt.ylim([0, 50])\n",
    "       plt.xlabel('Year'); plt.ylabel(('# of States With Differences Exceeding ' + str(threshold) + '% Threshold'))\n",
    "       plt.title(('Socioeconomic Scenario Divergence Across States: Threshold = ' + str(threshold) + '%'))\n",
    "    if process_states == False:\n",
    "       plt.ylim([0, 55])\n",
    "       plt.xlabel('Year'); plt.ylabel(('# of BAs With Differences Exceeding ' + str(threshold) + '% Threshold'))\n",
    "       plt.title(('Socioeconomic Scenario Divergence Across BAs: Threshold = ' + str(threshold) + '%'))\n",
    "    plt.grid(True)\n",
    "    plt.legend(prop={'size': 12})\n",
    "    \n",
    "    plt.subplot(222)\n",
    "    plt.plot(-5, 0, color='k', linestyle='-', label='Mean Load', linewidth=2)\n",
    "    plt.plot(-5, 0, color='k', linestyle=':', label='Peak Load', linewidth=2)\n",
    "    plt.plot(rcp_div_a['Year'], rcp_div_a['Mean_Breaks'], color='forestgreen', linestyle='-', label=(rcp_div_a_one + ' vs ' + rcp_div_a_two), linewidth=2)\n",
    "    plt.plot(rcp_div_b['Year'], rcp_div_b['Mean_Breaks'], color='blueviolet', linestyle='-', label=(rcp_div_b_one + ' vs ' + rcp_div_b_two), linewidth=2)\n",
    "    plt.plot(rcp_div_c['Year'], rcp_div_c['Mean_Breaks'], color='goldenrod', linestyle='-', label=(rcp_div_c_one + ' vs ' + rcp_div_c_two), linewidth=2)\n",
    "    plt.plot(rcp_div_d['Year'], rcp_div_d['Mean_Breaks'], color='lightcoral', linestyle='-', label=(rcp_div_d_one + ' vs ' + rcp_div_d_two), linewidth=2)\n",
    "    plt.plot(rcp_div_a['Year'], rcp_div_a['Peak_Breaks'], color='forestgreen', linestyle=':', linewidth=2)\n",
    "    plt.plot(rcp_div_b['Year'], rcp_div_b['Peak_Breaks'], color='blueviolet', linestyle=':', linewidth=2)\n",
    "    plt.plot(rcp_div_c['Year'], rcp_div_c['Peak_Breaks'], color='goldenrod', linestyle=':', linewidth=2)\n",
    "    plt.plot(rcp_div_d['Year'], rcp_div_d['Peak_Breaks'], color='lightcoral', linestyle=':', linewidth=2)\n",
    "    plt.xlim([2020, 2095]);\n",
    "    if process_states == True:\n",
    "       plt.ylim([0, 50])\n",
    "       plt.xlabel('Year'); plt.ylabel(('# of States With Differences Exceeding ' + str(threshold) + '% Threshold'))\n",
    "       plt.title(('Climate Scenario Divergence Across States: Threshold = ' + str(threshold) + '%'))\n",
    "    if process_states == False:\n",
    "       plt.ylim([0, 55])\n",
    "       plt.xlabel('Year'); plt.ylabel(('# of BAs With Differences Exceeding ' + str(threshold) + '% Threshold'))\n",
    "       plt.title(('Climate Scenario Divergence Across BAs: Threshold = ' + str(threshold) + '%'))\n",
    "    plt.grid(True)\n",
    "    plt.legend(prop={'size': 12})\n",
    "    \n",
    "    plt.subplot(223)\n",
    "    plt.plot(-5, 0, color='k', linestyle='-', label='Mean Load', linewidth=2)\n",
    "    plt.plot(-5, 0, color='k', linestyle=':', label='Peak Load', linewidth=2)\n",
    "    plt.plot(gcm_div_a['Year'], gcm_div_a['Mean_Breaks'], color='brown', linestyle='-', label=(gcm_div_a_one + ' vs ' + gcm_div_a_two), linewidth=2)\n",
    "    plt.plot(gcm_div_b['Year'], gcm_div_b['Mean_Breaks'], color='gold', linestyle='-', label=(gcm_div_b_one + ' vs ' + gcm_div_b_two), linewidth=2)\n",
    "    plt.plot(gcm_div_c['Year'], gcm_div_c['Mean_Breaks'], color='skyblue', linestyle='-', label=(gcm_div_c_one + ' vs ' + gcm_div_c_two), linewidth=2)\n",
    "    plt.plot(gcm_div_d['Year'], gcm_div_d['Mean_Breaks'], color='magenta', linestyle='-', label=(gcm_div_d_one + ' vs ' + gcm_div_d_two), linewidth=2)\n",
    "    plt.plot(gcm_div_a['Year'], gcm_div_a['Peak_Breaks'], color='brown', linestyle=':', linewidth=2)\n",
    "    plt.plot(gcm_div_b['Year'], gcm_div_b['Peak_Breaks'], color='gold', linestyle=':', linewidth=2)\n",
    "    plt.plot(gcm_div_c['Year'], gcm_div_c['Peak_Breaks'], color='skyblue', linestyle=':', linewidth=2)\n",
    "    plt.plot(gcm_div_d['Year'], gcm_div_d['Peak_Breaks'], color='magenta', linestyle=':', linewidth=2)\n",
    "    plt.xlim([2020, 2095]);\n",
    "    if process_states == True:\n",
    "       plt.ylim([0, 50])\n",
    "       plt.xlabel('Year'); plt.ylabel(('# of States With Differences Exceeding ' + str(threshold) + '% Threshold'))\n",
    "       plt.title(('Climate Model Divergence Across States: Threshold = ' + str(threshold) + '%'))\n",
    "    if process_states == False:\n",
    "       plt.ylim([0, 55])\n",
    "       plt.xlabel('Year'); plt.ylabel(('# of BAs With Differences Exceeding ' + str(threshold) + '% Threshold'))\n",
    "       plt.title(('Climate Model Divergence Across BAs: Threshold = ' + str(threshold) + '%'))\n",
    "    plt.grid(True)\n",
    "    plt.legend(prop={'size': 12})\n",
    "    \n",
    "    # If the \"save_images\" flag is set to true then save the plot to a .png file:\n",
    "    if save_images == True:\n",
    "       if process_states == True:\n",
    "          filename = ('All_State_Load_Divergence_' + str(threshold) + '%_Threshold.png')\n",
    "       if process_states == False:\n",
    "          filename = ('All_BA_Load_Divergence_' + str(threshold) + '%_Threshold.png')\n",
    "       plt.savefig(os.path.join(image_output_dir, filename), dpi=image_resolution, bbox_inches='tight', facecolor='white')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd207294-c063-40a3-845a-9dca194b7cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_group_point_of_divergence(data_input_dir = data_output_dir, \n",
    "                               threshold = 7.5, \n",
    "                               process_states = False,\n",
    "                               image_output_dir = image_output_dir, \n",
    "                               image_resolution = 300, \n",
    "                               save_images = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7407b2-378c-465e-8eb4-c00f1e93525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot the maps of year of divergence by state:\n",
    "def plot_state_divergence_maps(data_input_dir: str, threshold: int, scenario_one: str, scenario_two: str, title_str: str, image_output_dir: str, image_resolution: int, save_images=False):\n",
    "    \n",
    "    # Read in the county shapefile and reassign the 'FIPS' variable as integers:\n",
    "    states_df = gpd.read_file(os.path.join(data_input_dir, r'cb_2018_us_state_5m', r'cb_2018_us_state_5m.shp')).rename(columns={'NAME': 'Name'})\n",
    "       \n",
    "    # Load in the time series:\n",
    "    div_df = pd.read_csv(data_input_dir + 'State_Divergence_Years_' + str(threshold) + '.csv') \n",
    "        \n",
    "    # Subset the data to only the scenario you want to plot:\n",
    "    subset_df = div_df.loc[(div_df['Scenario_One'] == scenario_one) & (div_df['Scenario_Two'] == scenario_two)]\n",
    "    \n",
    "    # Merge the subset_df and states_df together using state names to join them:\n",
    "    states_df = states_df.merge(subset_df, on='Name', how='left')\n",
    "\n",
    "    # Set the colormap:\n",
    "    colors = plt.get_cmap('YlGnBu', 15)\n",
    "        \n",
    "    # Create the figure:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(25, 10))\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"1%\", pad=0)\n",
    "    ax1 = states_df.plot(column='Mean_Break_Year', cmap=colors, \n",
    "                         missing_kwds = dict(color = \"lightgrey\"), \n",
    "                         vmin=2020, vmax=2095,\n",
    "                         ax=ax, cax=cax,\n",
    "                         edgecolor='k', linewidth=1,\n",
    "                         legend=True, legend_kwds={'label': ('First Year in Which Hourly Mean Load Differences Exceed ' + str(threshold) + '%'), 'orientation': 'vertical'})\n",
    "    ax1.set_xlim(-126.5, -66); ax1.set_xlabel('Longitude');\n",
    "    ax1.set_ylim(24, 50); ax1.set_ylabel('Latitude');\n",
    "    ax1.set_title((title_str + scenario_one + ' vs. ' + scenario_two + ', Mean Load, Threshold = ' + str(threshold) + '%'))\n",
    "    \n",
    "    # If the \"save_images\" flag is set to true then save the plot to a .png file:\n",
    "    if save_images == True:\n",
    "       filename = ('Divergence_Map_Mean_' + str(threshold) + '%_Threshold_' + scenario_one + '_vs_' + scenario_two + '.png')\n",
    "       plt.savefig(os.path.join(image_output_dir, filename), dpi=image_resolution, bbox_inches='tight', facecolor='white')\n",
    "        \n",
    "    # Create the figure:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(25, 10))\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"1%\", pad=0)\n",
    "    ax1 = states_df.plot(column='Peak_Break_Year', cmap=colors, \n",
    "                         missing_kwds = dict(color = \"lightgrey\"), \n",
    "                         vmin=2020, vmax=2095,\n",
    "                         ax=ax, cax=cax,\n",
    "                         edgecolor='k', linewidth=1,\n",
    "                         legend=True, legend_kwds={'label': ('First Year in Which Hourly Peak Load Differences Exceed ' + str(threshold) + '%'), 'orientation': 'vertical'})\n",
    "    ax1.set_xlim(-126.5, -66); ax1.set_xlabel('Longitude');\n",
    "    ax1.set_ylim(24, 50); ax1.set_ylabel('Latitude');\n",
    "    ax1.set_title((title_str + scenario_one + ' vs. ' + scenario_two + ', Peak Load, Threshold = ' + str(threshold) + '%'))\n",
    "    \n",
    "    # If the \"save_images\" flag is set to true then save the plot to a .png file:\n",
    "    if save_images == True:\n",
    "       filename = ('Divergence_Map_Peak_' + str(threshold) + '%_Threshold_' + scenario_one + '_vs_' + scenario_two + '.png')\n",
    "       plt.savefig(os.path.join(image_output_dir, filename), dpi=image_resolution, bbox_inches='tight', facecolor='white')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2802e556-eef5-4a31-9c4d-e52051f08739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the groups of scenarios to plot:\n",
    "ssp_div_a_one = 'rcp45cooler_ssp5'; ssp_div_a_two = 'rcp45cooler_ssp3';\n",
    "ssp_div_b_one = 'rcp45hotter_ssp5'; ssp_div_b_two = 'rcp45hotter_ssp3';\n",
    "ssp_div_c_one = 'rcp85cooler_ssp5'; ssp_div_c_two = 'rcp85cooler_ssp3';\n",
    "ssp_div_d_one = 'rcp85hotter_ssp5'; ssp_div_d_two = 'rcp85hotter_ssp3';\n",
    "  \n",
    "rcp_div_a_one = 'rcp85cooler_ssp3'; rcp_div_a_two = 'rcp45cooler_ssp3'; \n",
    "rcp_div_b_one = 'rcp85hotter_ssp3'; rcp_div_b_two = 'rcp45hotter_ssp3'; \n",
    "rcp_div_c_one = 'rcp85cooler_ssp5'; rcp_div_c_two = 'rcp45cooler_ssp5'; \n",
    "rcp_div_d_one = 'rcp85hotter_ssp5'; rcp_div_d_two = 'rcp45hotter_ssp5';\n",
    "    \n",
    "gcm_div_a_one = 'rcp45hotter_ssp3'; gcm_div_a_two = 'rcp45cooler_ssp3'; \n",
    "gcm_div_b_one = 'rcp45hotter_ssp5'; gcm_div_b_two = 'rcp45cooler_ssp5'; \n",
    "gcm_div_c_one = 'rcp85hotter_ssp3'; gcm_div_c_two = 'rcp85cooler_ssp3'; \n",
    "gcm_div_d_one = 'rcp85hotter_ssp5'; gcm_div_d_two = 'rcp85cooler_ssp5';\n",
    "\n",
    "plot_state_divergence_maps(data_input_dir = data_output_dir, \n",
    "                           threshold = 5,\n",
    "                           scenario_one = ssp_div_a_one, \n",
    "                           scenario_two = ssp_div_a_two,\n",
    "                           title_str = 'Socioeconomic Scenario Divergence [SSP3 vs SSP5]: ',\n",
    "                           #title_str = 'Climate Scenario Divergence [RCP 4.5 vs RCP 8.5]: ',\n",
    "                           #title_str = 'Climate Model Divergence [Hotter vs Colder Models]: ',\n",
    "                           image_output_dir = image_output_dir, \n",
    "                           image_resolution = 300, \n",
    "                           save_images = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38e6c6d-8983-4b44-8901-68b335b9c30f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.9.15_std",
   "language": "python",
   "name": "py3.9.15_std"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
